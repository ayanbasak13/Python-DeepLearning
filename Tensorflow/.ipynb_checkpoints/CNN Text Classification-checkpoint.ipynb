{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = 0\n",
    "def read_data(filename):\n",
    "    global max_sent_length\n",
    "    questions = []\n",
    "    labels = []\n",
    "    fptr = open(filename,'r',encoding='latin-1')\n",
    "    lines = fptr.readlines()\n",
    "    for line in lines:\n",
    "        row_str = line.split(\":\")\n",
    "        lb,q = row_str[0],row_str[1]\n",
    "        q = q.lower()\n",
    "        labels.append(lb)\n",
    "        questions.append(q.split())        \n",
    "        if len(questions[-1])>max_sent_length:\n",
    "            max_sent_length = len(questions[-1])\n",
    "    return questions,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tQuestion 0: ['manner', 'how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?']\n",
      "\tLabel 0: DESC\n",
      "\n",
      "\tQuestion 1: ['cremat', 'what', 'films', 'featured', 'the', 'character', 'popeye', 'doyle', '?']\n",
      "\tLabel 1: ENTY\n",
      "\n",
      "\tQuestion 2: ['manner', 'how', 'can', 'i', 'find', 'a', 'list', 'of', 'celebrities', \"'\", 'real', 'names', '?']\n",
      "\tLabel 2: DESC\n",
      "\n",
      "\tQuestion 3: ['animal', 'what', 'fowl', 'grabs', 'the', 'spotlight', 'after', 'the', 'chinese', 'year', 'of', 'the', 'monkey', '?']\n",
      "\tLabel 3: ENTY\n",
      "\n",
      "\tQuestion 4: ['exp', 'what', 'is', 'the', 'full', 'form', 'of', '.com', '?']\n",
      "\tLabel 4: ABBR\n",
      "\n",
      "Max Sentence Length: 33\n",
      "\n",
      "Normalizing all sentences to same length now.....\n"
     ]
    }
   ],
   "source": [
    "global train_questions,train_labels\n",
    "global test_questions,test_labels\n",
    "\n",
    "\n",
    "train_questions,train_labels = read_data(os.path.join('question-classif-data','trec-train-1000.txt'))\n",
    "assert len(train_questions)==len(train_labels)\n",
    "\n",
    "test_questions,test_labels = read_data(os.path.join('question-classif-data','trec-test.txt'))\n",
    "assert len(test_questions)==len(test_labels)\n",
    "for j in range(5):\n",
    "    print('\\tQuestion %d: %s' %(j,train_questions[j]))\n",
    "    print('\\tLabel %d: %s\\n'%(j,train_labels[j]))\n",
    "        \n",
    "print('Max Sentence Length: %d'%max_sent_length)\n",
    "print('\\nNormalizing all sentences to same length now.....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain questions normalized\n",
      "\tTest questions normalized\n",
      "\t\tSample test question: %s ['dist', 'how', 'far', 'is', 'it', 'from', 'denver', 'to', 'aspen', '?', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "for qi,que in enumerate(train_questions):\n",
    "    for _ in range(max_sent_length-len(que)):\n",
    "        que.append('PAD')\n",
    "    assert len(que)==max_sent_length\n",
    "    train_questions[qi] = que\n",
    "print('\\tTrain questions normalized')\n",
    "for qi,que in enumerate(test_questions):\n",
    "    for _ in range(max_sent_length-len(que)):\n",
    "        que.append('PAD')\n",
    "    assert len(que)==max_sent_length\n",
    "    test_questions[qi] = que\n",
    "print('\\tTest questions normalized')  \n",
    "print('\\t\\tSample test question: %s',test_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49500 Words found.\n",
      "Found 3369 words in the vocabulary. \n"
     ]
    }
   ],
   "source": [
    "# Make data Numerical\n",
    "\n",
    "def build_dataset(questions):\n",
    "    #print(questions)\n",
    "    words = []\n",
    "    data_list = []\n",
    "    count = []\n",
    "    for d in questions:\n",
    "        words.extend(d)\n",
    "    print('%d Words found.'%len(words))    \n",
    "    print('Found %d words in the vocabulary. '%len(collections.Counter(words).most_common()))\n",
    "    count.extend(collections.Counter(words).most_common())\n",
    "    #print(count)\n",
    "    #print(\"####\")\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    #for i in range (0,5) :\n",
    "        #print(str(list(dictionary.keys())[i]) + \":\" + str(dictionary[list(dictionary.keys())[i]]))\n",
    "    for d in questions:\n",
    "        data = list()\n",
    "        for word in d:\n",
    "            index = dictionary[word]        \n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "    #print(data_list)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary    \n",
    "    #return questions\n",
    "    \n",
    "#build_dataset(train_questions)\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "all_questions = list(train_questions)\n",
    "all_questions.extend(test_questions)\n",
    "\n",
    "all_question_ind, count, dictionary, reverse_dictionary = build_dataset(all_questions)\n",
    "#print(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words (count) [('PAD', 34407), ('?', 1454), ('the', 999), ('what', 963), ('is', 587)]\n",
      "0th entry in dictionary: %s PAD\n",
      "\n",
      "Sample data\n",
      "[38, 12, 19, 2288, 1564, 6, 28, 1734, 1369, 898, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[45, 3, 855, 1167, 2, 192, 1789, 1545, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Vocabulary size:  3369\n",
      "\n",
      "Train size:  1000\n",
      "Test size:  500\n"
     ]
    }
   ],
   "source": [
    "# Printing a few values(To check)\n",
    "\n",
    "print('All words (count)', count[:5])\n",
    "print('0th entry in dictionary: %s',reverse_dictionary[0])\n",
    "print('\\nSample data') \n",
    "print(all_question_ind[0])\n",
    "print(all_question_ind[1])\n",
    "\n",
    "print('\\nVocabulary size: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "\n",
    "print('\\nTrain size: ',len(train_questions))\n",
    "print('Test size: ',len(test_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
